{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "In this notebook I do text preprocessing and modeling using count vectors and topics as features.\n",
    "I also try to add parts of speech, user data, and polarity as features. I make most of my graphs and save the results of my modeling as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.metrics import classification_report,accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reads in a CSV file as a dataframe and drops null rows.\n",
    "def import_reviews(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        df.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "    df = df.dropna()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in the reviews and doing text processing on them.\n",
    "df = import_reviews('in_n_out_reviews')\n",
    "\n",
    "df_text = text_preprocessing(df,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the tokenized text to a dataframe for future use\n",
    "df_text.to_csv('tokenized_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processes text to be used in topic modeling. Tokenizes and adds n-grams. Also adds the length\n",
    "# of the review as a column.\n",
    "def text_preprocessing(df, ngram):\n",
    "    df['tokens'] = df['text'].apply(simple_preprocess)\n",
    "    df_text = df[['useful','text', 'tokens']]\n",
    "    df_text = df\n",
    "    df_text = df_text.reset_index()\n",
    "    df_text.drop(columns='index', inplace=True)\n",
    "    df_text = df_text[(df_text.tokens.str.len() > 5)]\n",
    "    df_text['useful'] = df_text['useful'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    df_text['length'] = df_text['text'].apply(len)\n",
    "    if ngram > 1:\n",
    "        df_text['grams'] = df_text['tokens'].apply(lambda x: find_ngrams(x, n=ngram))\n",
    "        df_text['modeling_text_list'] = df_text['tokens'] + df_text['grams']\n",
    "        df_text['modeling_text'] = df_text['modeling_text_list'].apply(lambda x:' '.join(x))\n",
    "    else:\n",
    "        df_text['modeling_text'] = df_text['tokens'].apply(lambda x:' '.join(x))\n",
    "    return df_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saves the results of my modeling to a csv\n",
    "def save_metrics(metric_list, df):\n",
    "    df.append(pd.DataFrame(metric_list))\n",
    "    df.to_csv('metrics_dataframe')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in some text, I would overwrite csv files as I updated them so I no longer have the original.\n",
    "df_topic_probs = import_reviews('lda_doc_topic_probs')\n",
    "\n",
    "df = import_reviews('tokenized_text')\n",
    "\n",
    "df_reviews = import_reviews('in_n_out_reviews')\n",
    "\n",
    "df_user = import_reviews('user_info_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to match up reviewer data with the review that they wrote. This was to add more features to model on.\n",
    "df_user.columns = [['useful_sum', 'review_count', 'yelping_since', 'cool_sum', 'funny_sum',\n",
    "       'compliment_cool', 'elite', 'user_id']]\n",
    "\n",
    "df_reviews_user = pd.merge(df_reviews, df_user, how='left', on=['user_id'], sort=False)\n",
    "\n",
    "df_user_with_text = df_reviews_user[['stars', 'text','useful_sum', 'review_count', 'yelping_since',\n",
    "                                       'cool_sum', 'funny_sum', 'compliment_cool', 'elite']]\n",
    "\n",
    "df_tokenized_user = text_preprocessing(df_user_with_text, 1)\n",
    "\n",
    "df_text_user = pd.merge(df, df_tokenized_user, how='left', on=['text'], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I was getting different lengths for the two dataframes so I began to suspect that something wasn't working as\n",
    "#intended\n",
    "len(df), len(df_text_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the day the user joined into the age of the account in days.\n",
    "def days_old(date):\n",
    "    date = datetime.datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    return (datetime.datetime.utcnow() - date).days\n",
    "df_text_user['account_age'] = df_text_user['yelping_since'].apply(days_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making an account_age column in days\n",
    "df_tokenized_user['account_age'] = df_tokenized_user['yelping_since'].apply(days_old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving my the user data matched with review dataframe.\n",
    "df_tokenized_user.to_csv('my_lst_hope')\n",
    "\n",
    "df_text_user.to_csv('user_info_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_topic_probs), len(df_text_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Attempting to include POS as a feature! Doesn't work very well\n",
    "\n",
    "# adjective = set([\"JJ\", \"JJR\", 'JJS'])\n",
    "# adverb = set([\"RB\", \"RBR\", 'RBS'])\n",
    "# adj_count = []\n",
    "# adv_count = []\n",
    "# for i in range(len(df)):\n",
    "#     adj, adv, pn = 0, 0, 0\n",
    "#     pos_list = TextBlob(df.iloc[i]['modeling_text']).pos_tags\n",
    "#     temp_dict = Counter(pos_list[1])\n",
    "#     for key in temp_dict:\n",
    "#         value = temp_dict[key]\n",
    "#         if key in adjective:\n",
    "#             adj += value\n",
    "#         elif key in adverb:\n",
    "#             adv += value\n",
    "#     adj_count.append(adj)\n",
    "#     adv_count.append(adv)\n",
    "# df_topic_probs['adjective'] = adj_count\n",
    "# df_topic_probs['adv_count'] = adv_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_topic_probs.groupby('useful')['adv_count', 'proper_n'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_probs.to_csv('lda_doc_topic_probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding polarity for each review with TextBlob\n",
    "df_topic_probs['polarity'] = df['modeling_text'].apply(lambda x: TextBlob(x).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_probs.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_topic_probs), len(df['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_topic_probs[df_topic_probs['polarity'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_probs = df_topic_probs.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_topic_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that creates ngrams\n",
    "def find_ngrams(input_list, n):\n",
    "    # Courtesy http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "    ngrams = zip(*[input_list[i:] for i in range(n)])\n",
    "    flattened_ngrams = list(map(lambda x: '_'.join(x), ngrams))\n",
    "    return flattened_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding custom stopwords\n",
    "stopword = set(stopwords.words('english'))\n",
    "stopword = stopword.union(set(['food', 'this', 'place', 'the', 'of', 'is', 'came', 'was', 'for', 'have', 'had'\n",
    "                           ,'and', 'get', 'one', 'food', 'guy','?','!','place', 'good', 'fries','burger', 'burgers',\n",
    "                            'got', 'eat','great', 'us', 'asked', 'service', 'back', 'time', 'like', 'vegas', 'go',\n",
    "                            'try', 'animal', 'style', 'double', 'good', 'just', 'always', 'location', 'fresh',\n",
    "                              'east', 'coast', 'order', 'ordered', 'fast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating count vectors\n",
    "vocabulary = set(itertools.chain.from_iterable(df_text['modeling_text_list']))\n",
    "vectorizer = CountVectorizer(vocabulary=vocabulary, stop_words= stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating an empty list to store results of my models\n",
    "metric_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Creates train and test data with the count vectors to be used for Multinomial Naive Bayes modelling\"\"\"\n",
    "# X_train, X_test, y_train, y_test = train_test_split(df_text['modeling_text'], df_text['useful'], test_size=0.3, random_state = 15)\n",
    "\n",
    "# # Create X, y vectors\n",
    "# X_train = vectorizer.fit_transform(X_train).todense()\n",
    "\n",
    "# X_test = vectorizer.transform(X_test).todense()\n",
    "\n",
    "# # Create, train model\n",
    "# nb = MultinomialNB()\n",
    "# nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scoreMNB=nb.predict_proba(X_test)[:,1]\n",
    "fpr_MNB, tpr_MNB,_ = roc_curve(y_test, y_scoreMNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MultinomilaNB\n",
    "y_prednb = nb.predict(X_test)\n",
    "scores = nb.score(X_test, y_test)\n",
    "nb.score(X_train,y_train), scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results to a dictionary\n",
    "metrics_MNB = (metrics_to_dict('MultinomialNB', , y_test, nb.predict(X_test),\n",
    "                                   nb.score(X_train, y_train), nb.score(X_test, y_test),\n",
    "                                   'Null', 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_topic_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding results to my existing results dataframe\n",
    "df_metric_list.append(pd.DataFrame(metrics_MNB, index=range(1))).to_csv('metrics_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dataframe to store gridsearch results\n",
    "df_metric_list = pd.read_csv('metrics_dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in LSI topic probabilities, add length of review and polarity for modelling.\n",
    "df_topic_probs = pd.read_csv('lsi_topic_probs')\n",
    "df_topic_probs['length'] = df_topic_probs_lda['length']\n",
    "df_topic_probs['polarity'] = df_topic_probs_lda['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_probs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperates X and y for modeling and creates train, test sets\n",
    "X = df_topic_probs[['3', '1', '2', 'length']]\n",
    "y = df_topic_probs_lda['useful']\n",
    "X = scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GausianNB for predicting with length and n-topics as features\n",
    "params = {'priors' : [None]}\n",
    "gb = GridSearchCV(GaussianNB(),params, scoring='roc_auc', cv=5)\n",
    "gb.fit(X_train, y_train)\n",
    "y_predgb = gb.predict(X_test)\n",
    "gb_scores = gb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_scoregb=gb.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probgb = gb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr,_ = roc_curve(y_test, y_scoregb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_scores, print(change_threshold(y_probgb, 0.55))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add results to list\n",
    "n=3\n",
    "metric_list.append(metrics_to_dict('GaussianNB', n, y_test, gb.predict(X_test),\n",
    "                                   gb.score(X_train, y_train), gb.score(X_test, y_test), \"Null\",1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search for logistic regression\n",
    "log_params = {'C': [ 1e-2,1e-1, 1,10,100],\n",
    "             'penalty': ['l1','l2']}\n",
    "log = GridSearchCV(LogisticRegression(), log_params, scoring='roc_auc', cv = 5)\n",
    "log.fit(X_train,np.ravel(y_train))\n",
    "y_scorelog=log.predict_proba(X_test)[:,1]\n",
    "y_problog = log.predict_proba(X_test)\n",
    "coefficients = list(log.best_estimator_.coef_)\n",
    "fpr_log, tpr_log,_ = roc_curve(y_test, y_scorelog)\n",
    "roc_auc = auc(fpr_log, tpr_log)\n",
    "log.score(X_train, y_train), log.score(X_test, y_test), log.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observing precision with different thresholds.\n",
    "print(change_threshold(y_problog, 0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add logistic regression metrics to list\n",
    "n=3\n",
    "metric_list.append(metrics_to_dict('Logistic Regression(lsi)', n, y_test,\n",
    "                                   log.predict(X_test), log.score(X_train, y_train), log.score(X_test, y_test),\n",
    "                                   log.best_params_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient boosting grid search\n",
    "gb_param = {'n_estimators' : [25,100,200],\n",
    "        'max_depth' : [2,3,10],\n",
    "        'learning_rate': [1e-2, 1e-1, 1]}\n",
    "gradboost = GridSearchCV(GradientBoostingClassifier(), gb_param,scoring = 'roc_auc', cv= 5)\n",
    "gradboost.fit(X_train, y_train)\n",
    "y_score_gboost=gradboost.predict_proba(X_test)[:,1]\n",
    "y_prob_gboost = gradboost.predict_proba(X_test)\n",
    "fpr_gboost, tpr_gboost,_ = roc_curve(y_test, y_score_gboost)\n",
    "roc_auc = auc(fpr_gboost, tpr_gboost)\n",
    "\n",
    "gradboost.score(X_train, y_train), gradboost.score(X_test, y_test), gradboost.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predgb = gradboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Observing precision with different thresholds\n",
    "print(change_threshold(y_prob_gboost, 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add gradient boosting metrics to list\n",
    "n = 3\n",
    "metric_list.append(metrics_to_dict('Gradient Boosting(lsi)', n, y_test, gradboost.predict(X_test),\n",
    "                                   gradboost.score(X_train, y_train), gradboost.score(X_test, y_test),\n",
    "                                   gradboost.best_params_, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving results\n",
    "save_metrics(metric_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the classification report for different thresholds, n.\n",
    "def change_threshold(y_prob, n):\n",
    "    y_pred = [1 if x >= n else 0 for x in y_prob[:, 1]]\n",
    "    return classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a dictionary of metrics for each algorithm\n",
    "def metrics_to_dict(algorithm, n_topics, y_test, y_pred, auc_train, auc_test, params, sa):\n",
    "    if not sa:\n",
    "        sa = 0\n",
    "    results = {}\n",
    "    prec = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test,y_pred)\n",
    "    f1 = metrics.f1_score(y_test,y_pred)\n",
    "    results['Algorithm'] = algorithm\n",
    "    results['Parameters'] = params\n",
    "    results['Number of Topics'] = n_topics\n",
    "    results['Polarity'] = sa\n",
    "    results['Auc_train'] = auc_train\n",
    "    results['Auc_test'] = auc_test\n",
    "    results['Precision'] = prec\n",
    "    results['Recall'] = recall\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Making AUC curve for all the algorithms I used.\n",
    "plt.figure(figsize=[10,8])\n",
    "plt.plot([0,1],[0,1])\n",
    "\n",
    "plt.plot(fpr,tpr, label='Gaussian Naive Bayes', color = 'grey')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.plot(fpr_MNB,tpr_MNB, label='Multinomial Naive Bayes', color = 'grey')\n",
    "\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.plot(fpr_log,tpr_log, label='Logistic Regression', color = 'red')\n",
    "\n",
    "plt.plot([0,1],[0,1])\n",
    "plt.plot(fpr_gboost,tpr_gboost, label='Gradient Boosting', color = 'grey')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_text.groupby('useful').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting by AUC scores to see which algorithm performed the best and with which hyperparameters\n",
    "pd.DataFrame(metric_list).sort_values('Auc_test', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metric_list.fillna(0).sort_values('Auc_test', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making confusion matrix\n",
    "class_names = ['Not Useful','Useful']\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating confusion matrix.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion Matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
