{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook I did topic modeling with each review's TF-IDF instead of count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = pd.read_csv('tokenized_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = pd.read_csv('user_info_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation.iloc[0]['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "stopword = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding custom stopwords\n",
    "stopword = stopword.union(set(['food', 'this', 'place', 'the', 'of', 'is', 'came', 'was', 'for', 'have', 'had'\n",
    "                           ,'and', 'get', 'one', 'food', 'guy','?','!','place', 'good', 'fries','burger', 'burgers',\n",
    "                            'got', 'eat','great', 'us', 'asked', 'service', 'back', 'time', 'like', 'vegas', 'go',\n",
    "                            'try', 'animal', 'style', 'double', 'good', 'just', 'always', 'location', 'fresh',\n",
    "                              'east', 'coast', 'order', 'ordered', 'fast']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns tokens for each review\n",
    "def  chapter_reader():\n",
    "    for i in observation['text']:\n",
    "        yield (x for x in \n",
    "            gensim.utils.tokenize(i, lowercase=True, deacc=True, \n",
    "                                  errors=\"ignore\")\n",
    "            if (x not in stopword) and len(x) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "dictionary = gensim.corpora.Dictionary(chapter_reader())\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8, keep_n=100000)\n",
    "for values in chapter_reader():\n",
    "    corpus.append(dictionary.doc2bow(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prints most frequent word for each review\n",
    "for i,vector in enumerate(corpus):\n",
    "    try:\n",
    "        most_index, most_count = max(vector, key=lambda item: item[1])\n",
    "    except ValueError:\n",
    "        continue\n",
    "    print( \"Review \" + str(i+1) + \" most used word: \",end='')\n",
    "    print( dictionary[most_index], most_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts tokens to their term TF-IDF\n",
    "tfidf = gensim.models.TfidfModel(corpus, normalize=True)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "# lsi_tfidf = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "# lsi_tfidf.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LDA modeling to create topics and print them.\n",
    "lda = gensim.models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=3)\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract document topic probabilities and save to csv with length of review and usefulness as columns\n",
    "# def extract_topic_prob():\n",
    "doc_topics = []\n",
    "probs = []\n",
    "for i in chapter_reader():\n",
    "    bow = dictionary.doc2bow(i)\n",
    "    doc_topics.append(lda.get_document_topics(bow))\n",
    "for i in doc_topics:\n",
    "    a = []\n",
    "    for j in i:\n",
    "        a.append(j[1])\n",
    "    probs.append(a)\n",
    "df_doc_probs = pd.DataFrame(probs)\n",
    "df_doc_probs = df_doc_probs.fillna(0)\n",
    "df_doc_probs['length'] = observation['length']\n",
    "df_doc_probs['useful'] = observation['useful']\n",
    "df_doc_probs.to_csv('lda_doc_topic_probs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Trying to add user data as columns and saving to csv\"\"\"\n",
    "# df_doc_probs['review_count'] = observation['review_count']\n",
    "# df_doc_probs['account_age'] = observation['account_age']\n",
    "# df_doc_probs['useful_sum'] = observation['useful_sum']\n",
    "# df_doc_probs['stars'] = observation['stars']\n",
    "# df_doc_probs['funny_sum'] = observation['funny_sum']\n",
    "# df_doc_probs['compliment_cool'] = observation['compliment_cool']\n",
    "# df_doc_probs.to_csv('topic_probs_desperate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_probs.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_doc_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tried LSI just to see if topics would be similar\n",
    "lsi = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=3)\n",
    "lsi.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
